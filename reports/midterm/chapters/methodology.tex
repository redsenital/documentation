\chapter{Methodology}

\section{Architectural Design Overview}

Red Sentinel is implemented using a microservice architecture. In this design, each major functional component of the system runs as an independent service. These services communicate via well-defined APIs and are coordinated by a central orchestration core—the "Core Module." This microservice approach ensures that each module is isolated, independently deployable, testable, and can be scaled or replaced without impacting the rest of the system—addressing concerns of maintainability, modularity, and system complexity typical in security tools.

To connect the ML-based payload generator with the rest of the system and other modules, a data pipeline architecture is used. As described in the literature on integrating ML pipelines with microservices, this setup supports data ingestion, preprocessing, model serving, asynchronous communication, and modular isolation of ML components from other services.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/arch.png}
    \caption{System Architecture Diagram}
    \label{fig:system_architecture}
\end{figure}

\subsection{Core Module}

The Core Module serves as the orchestration layer and primary entry point for Red Sentinel. Implemented using NestJS, a progressive Node.js framework, it provides the foundational infrastructure for coordinating distributed microservices, managing workflow execution, and ensuring system reliability. The module operates as a RESTful API gateway that bridges external security researchers and internal ML-powered services.

\subsubsection{Architectural Role and Responsibilities}

The Core Module fulfills multiple critical roles within the Red Sentinel ecosystem:

\begin{itemize}
    \item \textbf{API Gateway and Request Routing:} Exposes RESTful endpoints that accept target URLs and scanning parameters from clients. Routes requests to appropriate downstream services based on workflow stage and service availability.
    
    \item \textbf{Input Validation and Security Enforcement:} Implements comprehensive validation logic to ensure all user-provided inputs conform to security policies. This includes URL structure validation, protocol whitelisting (HTTP/HTTPS only), and Server-Side Request Forgery (SSRF) prevention through private IP address blocking.
    
    \item \textbf{Workflow Orchestration:} Manages the multi-stage pipeline from initial target submission through context extraction, payload generation, obfuscation, and result aggregation. Coordinates asynchronous communication between services using message queues or direct HTTP invocation.
    
    \item \textbf{State Management:} Maintains scanning session state, tracks request progress, and persists intermediate results for fault tolerance and debugging. Implements distributed caching to reduce redundant operations.
    
    \item \textbf{Error Handling and Recovery:} Provides comprehensive error handling with typed exception filters, automatic retry mechanisms for transient failures, and graceful degradation when dependent services are unavailable.
    
    \item \textbf{Logging and Observability:} Generates structured logs with unique request identifiers for distributed tracing. Exposes metrics endpoints compatible with Prometheus for monitoring request rates, latency distributions, and error frequencies.
\end{itemize}

\subsubsection{Step One: Target Input and Page Retrieval}

The first operational step within the Core Module focuses on accepting target URLs, performing security validation, and retrieving raw HTML content. This foundational layer establishes the data pipeline upon which all subsequent analysis depends.

\paragraph{Input Validation}

When a client submits a target URL via the \texttt{POST /api/v1/gateway/fetch} endpoint, the system performs rigorous validation:

\begin{enumerate}
    \item \textbf{URL Structure Validation:} Parses the URL to ensure it contains all required components (protocol, hostname) and conforms to RFC 3986 standards. Rejects malformed URLs with descriptive error messages.
    
    \item \textbf{Protocol Enforcement:} Accepts only HTTP and HTTPS protocols. Rejects potentially dangerous schemes such as \texttt{file://}, \texttt{ftp://}, \texttt{javascript:}, and \texttt{data:} to prevent local file access and code injection attacks.
    
    \item \textbf{Length Constraints:} Enforces a maximum URL length of 2048 characters to prevent buffer overflow vulnerabilities and resource exhaustion.
    
    \item \textbf{DNS Resolution:} Resolves the target hostname to its IP address using standard DNS lookup mechanisms before initiating HTTP requests.
\end{enumerate}

\paragraph{SSRF Prevention}

To prevent Server-Side Request Forgery attacks, the Core Module implements IP address filtering that blocks requests to private network ranges:

\begin{itemize}
    \item \textbf{IPv4 Private Ranges:} \texttt{10.0.0.0/8}, \texttt{172.16.0.0/12}, \texttt{192.168.0.0/16} (RFC 1918)
    \item \textbf{Loopback Addresses:} \texttt{127.0.0.0/8} (IPv4), \texttt{::1/128} (IPv6)
    \item \textbf{Link-Local Addresses:} \texttt{169.254.0.0/16} (IPv4), \texttt{fe80::/10} (IPv6)
    \item \textbf{Reserved Ranges:} Multicast, broadcast, and experimental address spaces
\end{itemize}

Requests targeting these ranges are rejected with HTTP 403 Forbidden status, preventing attackers from scanning internal infrastructure or accessing cloud metadata endpoints.

\paragraph{HTTP Request Execution}

Upon successful validation, the Core Module configures an HTTP client with security-hardened settings:

\begin{itemize}
    \item \textbf{Timeout Configuration:} Enforces a default timeout of 30 seconds to prevent indefinite hanging. Configurable through request parameters within the range of 1-60 seconds.
    
    \item \textbf{Redirect Handling:} Follows HTTP redirects automatically up to a maximum of 5 redirects. Prevents redirect loops and excessive chain following that could indicate malicious targets.
    
    \item \textbf{TLS/SSL Validation:} Validates SSL certificates for HTTPS requests, rejecting self-signed certificates in production mode to ensure authenticity.
    
    \item \textbf{User-Agent Identification:} Sets a custom User-Agent header (\texttt{RedSentinel/1.0 (Security Scanner)}) for ethical disclosure, allowing site administrators to identify and control scanner access.
    
    \item \textbf{Content Size Limits:} Enforces a maximum response size of 10MB to prevent memory exhaustion attacks. Aborts downloads that exceed this threshold.
\end{itemize}

\paragraph{Response Processing}

After successfully retrieving the target page, the Core Module processes the HTTP response:

\begin{enumerate}
    \item \textbf{Status Code Capture:} Records the final HTTP status code (e.g., 200, 301, 404) for diagnostic purposes.
    
    \item \textbf{Header Extraction:} Captures relevant response headers including \texttt{Content-Type}, \texttt{Content-Length}, \texttt{Server}, and security headers (\texttt{X-XSS-Protection}, \texttt{Content-Security-Policy}).
    
    \item \textbf{Content Validation:} Verifies the \texttt{Content-Type} header indicates HTML content (\texttt{text/html}). Non-HTML responses are logged but not rejected, as some applications serve HTML with incorrect MIME types.
    
    \item \textbf{HTML Body Extraction:} Reads the complete response body as text, preserving all whitespace, encoding, and formatting for accurate downstream analysis.
\end{enumerate}

\paragraph{Response Standardization}

The Core Module returns a standardized JSON response structure for both successful and failed operations:

\begin{verbatim}
{
  "success": true,
  "data": {
    "targetUrl": "https://example.com/search",
    "finalUrl": "https://www.example.com/search",
    "statusCode": 200,
    "contentType": "text/html; charset=UTF-8",
    "contentLength": 45678,
    "html": "<!DOCTYPE html><html>...</html>",
    "redirectCount": 1,
    "fetchDuration": 1234
  },
  "metadata": {
    "timestamp": "2025-12-10T14:32:15.789Z",
    "requestId": "req_7f8a9b1c",
    "version": "1.0.0"
  }
}
\end{verbatim}

This consistent response format simplifies integration with downstream services and enables automated result processing.

\paragraph{Error Handling and Logging}

The Core Module implements comprehensive error handling with specific error codes for common failure scenarios:

\begin{itemize}
    \item \texttt{INVALID\_URL\_FORMAT}: Malformed URL structure
    \item \texttt{UNSUPPORTED\_PROTOCOL}: Non-HTTP/HTTPS protocol detected
    \item \texttt{PRIVATE\_IP\_BLOCKED}: SSRF prevention triggered
    \item \texttt{FETCH\_TIMEOUT}: Request exceeded timeout duration
    \item \texttt{CONTENT\_TOO\_LARGE}: Response size exceeded 10MB limit
    \item \texttt{TOO\_MANY\_REDIRECTS}: Redirect count exceeded maximum
\end{itemize}

All requests are logged with structured JSON formatting, including request IDs for distributed tracing, target URLs (sanitized to remove credentials), execution duration, and error details. Logs are categorized by severity (INFO, WARN, ERROR) and exported to centralized logging infrastructure for monitoring and auditing.

\subsubsection{Technology Stack and Implementation}

The Core Module leverages modern TypeScript-based technologies for reliability and maintainability:

\begin{itemize}
    \item \textbf{Framework:} NestJS 10.x with TypeScript 5.x for strong typing and compile-time safety
    \item \textbf{HTTP Client:} Axios with configurable timeout, redirect, and proxy support
    \item \textbf{Validation:} class-validator and class-transformer for declarative DTO validation
    \item \textbf{Logging:} Winston with JSON formatting for structured logging
    \item \textbf{Testing:} Jest for unit tests, Supertest for integration tests
    \item \textbf{Documentation:} Swagger/OpenAPI for automated API documentation
\end{itemize}

The modular architecture enables independent testing, deployment, and scaling of the Core Module, supporting Red Sentinel's microservice design philosophy.

\subsection{Context Module}

The Context Module is responsible for analyzing target web applications to identify injection points and extract contextual information necessary for generating appropriate payloads. This module performs:

\begin{itemize}
    \item \textbf{HTML Parsing:} Analyzes the Document Object Model (DOM) structure to identify form inputs, URL parameters, and dynamic content areas.
    
    \item \textbf{JavaScript Analysis:} Examines inline scripts and external JavaScript files to detect potential injection contexts within script blocks.
    
    \item \textbf{Context Classification:} Categorizes each identified injection point according to its syntactic context (HTML attribute, JavaScript string, event handler, URL parameter, etc.).
    
    \item \textbf{Sanitization Detection:} Attempts to identify client-side and server-side filtering mechanisms by testing basic payloads and analyzing responses.
    
    \item \textbf{Context Metadata Generation:} Produces structured metadata including context type, surrounding code, expected encoding, and sanitization indicators.
\end{itemize}

This metadata is formatted and passed to the Payload Handler to guide context-aware payload generation.

\subsection{Payload Handler}

The Payload Handler consists of two primary models working in tandem: the Payload Generator and the Obfuscation Model.

\subsubsection{Payload Generator}

The Payload Generator is responsible for producing syntactically valid, context-adapted, and execution-ready XSS attack strings. Red Sentinel employs a transformer-based encoder-decoder architecture trained specifically for cross-site scripting contexts and adversarial behavior.

The transformer model is trained on a dataset formed by pairs of sanitized script structures provided by the Core Module and the corresponding XSS payloads. The generator is built on an encoder-decoder transformer network, chosen because of its exceptional ability to model long-range dependencies and handle complex code sequences. The model comprises:

\begin{itemize}
    \item \textbf{Byte-Level Tokenizer}
    
    Instead of word- or character-level tokenization, Red Sentinel uses a byte-level tokenizer (similar to Byte-Pair Encoding) to ensure that all characters—including \texttt{<}, \texttt{>}, \texttt{'}, \texttt{"}, \texttt{(}, \texttt{)}, \texttt{\textbackslash}, and Unicode variants—are preserved without normalization. This is crucial because minor character transformations can change the exploitability of an XSS payload.
    
    \item \textbf{Encoder}
    
    The encoder receives the attack type, payload type, and contextual information extracted by the Context Module. The encoder produces a contextual embedding that expresses the syntactic and semantic constraints of the injection point.
    
    \item \textbf{Decoder}
    
    The decoder autoregressively generates an attack string conditioned on the encoder output. It learns patterns for escaping contexts, invoking JavaScript execution, nesting HTML/JS structures, and exploiting browser quirks. It is designed to avoid generating malformed payloads by learning valid grammar structures from curated training data.
    
    \item \textbf{Attention Mechanisms}
    
    Attention heads enable the model to correlate specific positions in the context (e.g., inside quotes, inside script tags) with correct exploit strategies. This facilitates generation of highly specific payloads such as:
    
    \begin{itemize}
        \item Attribute breakouts: \texttt{"><svg/onload=alert(1)>}
        \item Script-block injections: \texttt{';alert(1);//}
        \item URL-based injections: \texttt{javascript:alert(1)}
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/transformer.png}
    \caption{Transformer Model Structure}
    \label{fig:transformer_model}
\end{figure}

\subsubsection{Obfuscation Model}

Modern defensive systems often rely on pattern matching or signature detection, which is particularly vulnerable to obfuscation. Therefore, incorporating systematic obfuscation increases the likelihood of discovering hidden vulnerabilities and evaluating the true resilience of a target.

\paragraph{Encoding-Based Obfuscation}

This class manipulates the representation of characters while preserving their semantics at runtime:

\begin{itemize}
    \item URL encoding (percent-encoding): \texttt{\%3Cscript\%3E}
    \item HTML entity encoding: \texttt{\&\#x3C;script\&\#x3E;}
    \item Unicode homoglyphs: Alternative representations of common symbols
    \item Base64 wrapper techniques: e.g., \texttt{eval(atob("YWxlcnQoMSk="))}
\end{itemize}

These methods exploit weaknesses in sanitization routines that decode values inconsistently.

\paragraph{Structural Obfuscation}

Structural modification alters the payload's syntax without changing its effect:

\begin{itemize}
    \item String splitting: \texttt{a = "al" + "ert"; window[a](1)}
    \item Wrapped event handlers: \texttt{<img src=x onerror=\%61\%6c\%65\%72\%74(1)>}
    \item Nonstandard tag nesting
    \item Junk insertion (harmless characters or comments)
\end{itemize}

This category is particularly effective against filter engines that scan for simple signatures (e.g., \texttt{alert}).

\paragraph{JavaScript-Based Dynamic Obfuscation}

A more advanced transformation relies on runtime reconstruction:

\begin{itemize}
    \item Using constructor functions: \texttt{Function("al"+"ert(1)")()}
    \item Indirect invocation using event stacks
    \item Using proxies or dynamically generated DOM nodes to execute embedded code
\end{itemize}

These methods exploit JavaScript's dynamic nature to evade static analysis.

\paragraph{Obfuscation Module Overview}

The Obfuscation Module integrates into the system workflow as follows:

\begin{enumerate}
    \item The Payload Generator produces a set of base payloads
    \item Payloads are sent to the Obfuscation Service when enabled
    \item The transformed payloads are injected into the target applications
    \item All variants and their execution results are logged
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/flow.png}
    \caption{Payload Handler Flow Structure}
    \label{fig:payload_flow}
\end{figure}

Together, the Payload Generator and Obfuscation Module form the intelligent core of Red Sentinel. The generator creates contextually accurate, execution-ready XSS payloads, while the obfuscation system transforms them to evade filters and explore deeper vulnerabilities. Their microservice design ensures modularity, scalability, and extensibility, supporting Red Sentinel's mission of providing advanced, ML-driven offensive security capabilities.

\section{Dataset Preparation and Preprocessing}

A high-quality dataset is essential for training the transformer-based payload generator. The dataset construction process involved several stages:

\subsection{Data Collection}

Payload data was collected from multiple sources:

\begin{enumerate}
    \item \textbf{Open-Source Repositories:} XSS payload collections from GitHub repositories such as PayloadsAllTheThings, XSS Hunter, and OWASP's XSS Filter Evasion Cheat Sheet
    
    \item \textbf{Academic Publications:} Payloads extracted from research papers on XSS vulnerability detection
    
    \item \textbf{Vulnerability Databases:} Real-world exploit examples from CVE databases and security advisories
    
    \item \textbf{Synthetic Generation:} Rule-based generation of context-specific payloads for underrepresented categories
\end{enumerate}

\subsection{Context Labeling}

Each payload was manually or semi-automatically labeled with:

\begin{itemize}
    \item \textbf{Attack Class:} Reflected, Stored, DOM-based
    \item \textbf{Payload Type:} Event handler, script injection, attribute breakout, etc.
    \item \textbf{Context Type:} HTML attribute, JavaScript string, URL parameter, etc.
    \item \textbf{Risk Score:} Estimated severity and exploitability (0.0 to 1.0)
\end{itemize}

\subsection{Preprocessing Pipeline}

The preprocessing pipeline included:

\begin{enumerate}
    \item \textbf{Deduplication:} Removal of exact and near-duplicate payloads using hash-based and fuzzy matching
    
    \item \textbf{Normalization:} Standardization of whitespace, encoding formats, and character representations
    
    \item \textbf{Validation:} Syntactic validation to ensure payloads are well-formed
    
    \item \textbf{Tokenization:} Application of byte-level SentencePiece tokenization with byte fallback
    
    \item \textbf{Sequence Formatting:} Construction of input-output pairs with special tokens for context metadata
\end{enumerate}

Example formatted training instance:

\begin{verbatim}
Input: <ATTACK_CLASS>event-handler</ATTACK_CLASS>
       <PAYLOAD_TYPE>event-mouse</PAYLOAD_TYPE>
       <RISK>0.97</RISK>

Output: "><img src=1 onerror=prompt(document.domain)>
\end{verbatim}

\section{Activity Diagram}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/active.png}
    \caption{System Activity Diagram}
    \label{fig:activity_diagram}
\end{figure}

